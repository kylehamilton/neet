---
title: "Coding to `code::proof`: Practice fundamentals of automated tests for collaborative data analysis development"
subtitle: "How to go from no tests, to *whoa* tests!"
author: "sketch-draft by lead author, Charles T. Gray"
bibliography: references.bib
output:
tufte::tufte_html:
  number_sections: true
  toc: true

# output:
#   pdf_document:
#     number_sections: true
#     latex_engine: xelatex


---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.height = 2.3,
                      fig.width = 5,
                      fig.align = "center",
                      message=FALSE)

library(tufte)
library(ggplot2)

```

## Version of this manuscript

This manuscript (or manuscripts) will go through three phases:

1. A lead author sketch draft.
2. Implementation with the reproducibiliity team for [The repliCATS Project](https://replicats.research.unimelb.edu.au/) & version listing any team members who wish to co-author. 
3. Implementation of repliCATS procedures, explicitly, after we need not blind the algorithms for experimental ingegrity. Invitation for external co-authors to contribute and extend the content, review, and final submission. Possibly for useR2020. 

### Acknowledgments

People who have contributed feedback or assisted, some of whom might be co-authors on later drafts: Emily Riederer, Elise Gould, Hannah Fraser, Daniel Fryer, Aaron Willcox, David Wilkinson. Also, appreciation for Danielle Navarro, Thomas Pedersen and others on twitter who helped with the visualisation. 

# Coding to `code::proof` 

J. S. Bach's Contrapunctus I is not learnt in one sitting [todo: citation]. Indeed, for most pianists, to play the three-minute piece is the result of many months' diligent work, and still it will never feel *done*. To play well, a pianist must employ technqiue, but *how* a pianist is to achieve the desired technique is not clear [@chang_fundamentalspianopractice_2009a]. 

For a researcher performing an analysis using a computational tool such as R, it can be unclear when an analysis is *done*. And, while there are technical guides on *good enough* practice, it can feel overwhelming as to what to adopt and where. Particularly for an in-development algorithm, which is to say, an algorithm with scripts already started, and some results explored. 

This manuscript picks up from where Riederer left off with **RMarkdown-driven development** and suggests a  `code::register`ed (`c::r`ed[^2]) **test-driven workflow** for coding to doneness. This workflow provides a roadmap to completion for a packaged analysis with `code::proof`ed the manuscript [@grayCodeProofPrepare2019], provided measures of confidence in the implementation of the algorithm.   

[^2]: In this manuscript, `c::r` denotes `code::registration`, `c::r`ing denotes `code::registering`, and `c::r`ed denotes `code::resgistered` 

Musicians and, similarly, athletes, do not see themselves as having mastered a skill, but active practitioners of a craft [@galway_flute_1990]. To be a flautist is to practice, and to be an athlete is to train. For researcher developers, it can be hard to assess when an analysis is completed. In this manuscript, we consider the *practice* of test-driven analysis development as a means of *coding to doneness* for an algorithm, a workflow that faciliates the analyst achieving sufficient `code::proof` in their algorithm, confidence in the implementation of the aglorithm[@grayCodeProofPrepare2019]. 

# Questionable research practices in scientific computing

Algorithms are coded by people who practice code, and significant problems emerge when algorithms are treated as fixed artifacts, rather than one of the tools utilised by those who *practice* code. In Australia, a heartbreakingly-ongoing example of this kind of problem with the income-reporting data analyis algorithm that assesses if income welfare recipients have been overpaid entitlements. Crude averaging calculations have lead to ongoing incorrect debt notices issued, such as 20,000 people receiving "robodebt" notices for debts they did not owe [@mcilroy201720]. Problems in data analysis have real impacts on real people's lives. Perhaps, if the algorithm were considered a workflow practiced and monitored by a team of data scientists, rather than a static object, problems would not be persisting to this day [@karp_robodebtfederalcourt_2019]. Indeed, as noted in Wilson's testing primers (in development) for [RStudio](https://rstudio.cloud/learn/primers) [@_rstudiocloud_], 

> Almost all (92%) of the catastrophic system failures are the result of incorrect handling of non-fatal errors explicitly signaled in software. In 58% of the catastrophic failures, the underlying faults could easily have been detected through simple testing of error handling code [@yuan_simpletestingcan_2014].

We might view this as a computational instantiation of what Fraser *et al.* denote *questionable research practices* (QRPs). The QRPs Fraser *et al.* provide a taxonomy for refer to various practices in scientific methodology found in ecology and evolution, such as $p$-hacking, adjusting a model's specification to achieve a desired $p$-value, or *cherry picking*, failing to report variables or results that do not fit a desired narrative. QRPs are, importantly, often not a result of scientific malpractice, but a question of discipline-specific conventions established in bygone eras not translating well to data-driven research [@fraser_questionable_2018]. In scientific computing, as the robodebt example illustrates, similar and overlapping errors may occur. 

A consideration when providing recommendations of best practice is what me might reasonably expect of a researcher. Indeed, it is likely unrealistic to expect *best practices* in scientific computing [@wilson_best_2014], perhaps we would be better off asking for *good enough* practices [@wilson_good_2017] in scientific computing. For while researchers use computational algorithms in science every day, most of them are not trained in computational science. Even mathematicians and statisticians do a great deal of training at the blackboard, rather than the computer.    

Riederer identifies several qrps in scientific computing, for example, hardcoded values that interfere with another's ability to reproduce the computational results [@riederer_rmarkdowndrivendevelopment_2019]. As researcher software engineers, it behoves us to consider what are questionable research practices in software produced for data analyses. Version control and open code sharing via a platform such as GitHub, is one way to mitigate questionable research practices in scientific computing [@Bryan2017ExcuseMD]. There is also a growing literature on reproducible research compendia via packaged analyses [@marwick_packaging_2018; @wilson_good_2017].     

This manuscript contributes to this literature by focussing on workflows for using automated tests to move analysis a fully reproducible packaged research compendium. Furthermore, these workflosw assist the developer communicate what they have coded to others and their future self, as a means of mitigating questionable research practices in scientific computing. Above all, by providing a workflow for the *practice* of code, the developer anxiety is reduced by having the parameters as clearly defined as they can be from the start.

# Practice fundamentals

Chuang C. Chang's _Fundamentals of Piano Practice_ sets out to address a gap in piano pedagogy [@chang_fundamentalspianopractice_2009a]. A similar gap exists in the implementation of *good enough* practices [@wilson_good_2017], which is to say, what we might reasonably expect of a analyst, in reproducible computing for research data analyses. The objective is different from advanced pianism, but we characterise testing *practice*, as opposed to *techique*, analogous to how Change delineates between *piano* practice and technique. Through attempting to identify the fundamentals of automated testing for collaborative data analysis development, this manuscript aims to articulate the gap in understanding automated testing implementation for analysts. 

As Chang notes, whilst there a rich history of technical pedagogy, there is a dearth of guidance for pianists on _learning_ the technique [@chang_fundamentalspianopractice_2009a]. There are many canonical texts of pianistic technique pedagogy. Bach provides a pathway from small canons [todo: cite], to two-part inventions[todo: cite], three-part sinfonia, and the challenge of *The Well-Tempered Clavier* and *The Art of Fugue*. Bartok provides the *Mikrokosmos* [todo: ], and Czerny's *School of Velocity* [to do]. In each case, technical exercises of increasing difficulty are provided. In piano pedagogy, a _technical_ exercise isolates a particular aspect of pianistic technique [todo examples]. For example, [todo Czerny staccato]. Or, [todo voicing technique].

The dearth that Chang attempts to address is in pianistic _practice_ habits that will lead to succesful adoption of these techniques. In science, we might call this _work flow_ [todo cite]. 

> .. practically every piano learning method consists of showing students what to practice, and what kinds of techniques (runs, arpeggios, legato, staccato, trills, etc.) are needed. There are few instructions on how to practice order  to be able to play them, which is mostly left to the student and endless repetitions [@chang_fundamentalspianopractice_2009a].

Wilson et al. followed their work on _best practices_  in scientific computing [@wilson_best_2014], with a reflection on _good enough_ practices [@wilson_good_2017], in recognition that we must account for what we might reasonably request of practitioners of data analysis. In this manuscript, we consider one component of *good enough* practice in data analysis: *automated testing*. 

Automated tests are a formalised way of implementing checks that inputs and outputs of algorithsms are as expected [@wickham_r_2015]. Informative messages are output that assist the developer in identifying where code is not performing as expected. 

## Collaboration via automated testing 

At heart, automated tests are collaborative. This may be with others, but applies at least as much to a analyst approaching their own past work with which they have become unfamiliar, or have become anxious about some aspect of the work. Automated tests provide an efficient way of returning to and extending an analysis; anxiety is reduced by having defined outcomes to code explicitly for. 

Reproducible research compendia provide a means by which analysts can share their work so that it may be extended by others, and automated tests provide `code::proof` [@grayCodeProofPrepare2019], a measure of confidence in the algorithm for others. Hayes expresses concern about using algorithms published without automated tests [@hayes_testing_2019]. However, only one quarter of the largest repository of R packages, [The Comprehensive R Archive Network](https://cran.r-project.org/), have any automated tests [@hester_covr_2016], highlighting that, despite testing identified as a 'vital' part of data analysis [@wickham_r_2015], automated testing is yet to be widely adopted.

# A test-driven toolchain walkthrough for in-development analyses

From this section, we now consider the practicality of implementing tests through a *toolchain walkthrough*, an opinionated documetation of a scientific workflow, towards a measure of `code::proof`, confidence in the algorithm implemented [@grayCodeProofPrepare2019]. In particular, a toolchain walkthrough is a reflection of *one* workflow, whilst others, indeed, better, workflows might exist. This is in contrast to a comprehensive review of tools. Instead, a toolchain walkthrough ruminates on considerations *after* a set of tools have been chosen.

Toolchain walkthroughs aim to identify obstacles and advantages in implementation of scientific workflows. By necessity, a toolchain walkthrough is language specific, but, ideally, observations emerge that are useful for any appropriate language employed for data analysis, such as Python. This toolchain walkthrough examines the *process*, analogous to piano *practice*, of implementing tests, as opposed to defining comprehensively the nature of *good enough* tests, analogous to guidance on pianistic technique.

## Objective of this toolchain walkthrough

This toolchain walkthrough aims to provide guidance on implementing a test-driven workflow for an in-development analysis. Many analyses begin as scripts that develop [todo expan] [@riederer_rmarkdowndrivendevelopment_2019]. The central question of this manuscript is what constitutes a minimal level of testing for in-development analysis, where code is still being written and features implemented. Automated tests assist in time-consuming debugging tasks [@wickham_r_2015], but also in providing information with a developer who is unfamiliar with the code. 

This is a first effort in identifying the fundamentals of automated testing for the collaborative process of developing an analysis in R. Analgous to Riederer's *RMarkdown-driven development* [@riederer_rmarkdowndrivendevelopment_2019], which deconstructs the workflow of developing an analysis from .Rmd notebook-style reports to packaged analyses, we consider a set a computational tools that form a workflow to assist in the coherent development of automated tests for data analysis. This is an extension of the workflow suggestions provided in *R Packages*, with a specific focus on collaborative workflows in research.   

## Devops and assumed expertise

This toolchain walkthrough assumes a knowledge of R advanced enough to be using the language to be answering scientific research claims.

- tools used: testthat, neet, covr
- GitHub

## A toolchain walkthrough for `c::r` of...

This toolchain walkthrough for `c::r` reflects on the in-development practice of code to completion on a few different research projects. Consideration of sufficient `code::proof` of packages `varameta::`, `simeta::`, developed for the ouvre of doctoral work this manuscript contributes to. Another example project is considered, The repliCATS Project software of statistical aggregation methods, in particular `CATnap::` and `aggreCAT::`. 

For The repliCATS Project, the algorithms are not available until the research is complete, in order to blind the other team to the methodology. Although we cannot provide explicit code examples, the process is considered with respect to this project, as it is an example of collaborative in-development `c::r`.

### `varameta::`

The `varameta::` package is in-development analysis support software for a forthcoming manuscript, Meta-analysis of Medians. 

## Get an overview with `covr::`\label{sec: covr}

For an in-development packaged analysis, the `covr::` package provides a means of asssessing the testing coverage for each function. The analysis functions provided by `varameta::` were put on hold while signficant discussion was written for the thesis of which `varameta::` forms a part. Beginning with `covr::package_coverage`, enables us to get a more informed sense of the `code::proof` of this package, and what `code::proof` is still required to declare the analysis done.   

The `covr::` function, `::package_coverage`, provides the percentage of the lines of code run in tests and is a succinct method to get an at-a-glance sense coverage of testing scripts. But this assumes we have considered everything that might require testing. Whilst this is informative, it's not comprehensive. A `100%` in all functions in `covr::`, is informative, but is not done.  

Consider a function that takes a number $x$, and outputs $2\log(x)$. To demonstrate this we make a toy package `testtest::` comprising this single function.

```{r}
logfn <- function(x) {
  2 * log(x)
}

```

Now, we write a test that checks the function returns a numeric.

```{r}

library(testthat)

test_that("function returns numeric", {
  expect_is(logfn(3), "numeric")
})

```

And if we run `covr::package_coverage`, we get `100%` for overall package coverage.

```{r, eval=FALSE}
testtest Coverage: 100.00%
R/logfn.R: 100.00%

```

But if we were to add this test, for negative numbers. 

```{r error=TRUE}
test_that("log function works", {
  expect_is(logfn(-1), "numeric")
})

```

Then `covr::package_coverage` fails.

```{r eval=FALSE}
> covr::package_coverage()
Error: Failure in `/tmp/RtmpDjfm3d/R_LIBS654b3cfe6ca/testtest/testtest-tests/testthat.Rout.fail`
library(testthat)
> library(testtest)
> 
> test_check("testtest")
── 1. Failure: log function works (@test-logfn.R#10)  ──────────────────────────
any(is.na(thing_to_test)) isn't false.

── 2. Failure: log function works (@test-logfn.R#10)  ──────────────────────────
any(abs(as.numeric(thing_to_test)) == Inf) isn't false.

══ testthat results  ═══════════════════════════════════════════════════════════
[ OK: 8 | SKIPPED: 0 | WARNINGS: 1 | FAILED: 2 ]
1. Failure: log function works (@test-logfn.R#10) 
2. Failure: log function works (@test-logfn.R#10) 

Error: testthat unit tests failed
Execution halted
```

We pick up the long-neglected `varameta::` package at `70%` package coverage. But as we have seen above, this is informative, but not completely informative. From here we examine how we might use tests to achieve sufficient `code::proof`, confidence in the implementation of the algorithm.

``` r 
library(covr)
package_coverage("~/Documents/repos/varameta/")
#> varameta Coverage: 70.71%
#> R/dist_name.R: 0.00%
#> R/g_cauchy.R: 44.44%
#> R/g_norm.R: 71.43%
#> R/hozo_se.R: 92.31%
#> R/bland_mean.R: 100.00%
#> R/bland_se.R: 100.00%
#> R/effect_se.R: 100.00%
#> R/g_exp.R: 100.00%
#> R/g_lnorm.R: 100.00%
#> R/hozo_mean.R: 100.00%
#> R/wan_mean_C1.R: 100.00%
#> R/wan_mean_C2.R: 100.00%
#> R/wan_mean_C3.R: 100.00%
#> R/wan_se_C1.R: 100.00%
#> R/wan_se_C2.R: 100.00%
#> R/wan_se_C3.R: 100.00%
```

<sup>Created on 2020-02-01 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>

An excellent way to arrive at *done* for an algorithm is to define, even if only broadly, what *done* looks like *before* we set out. We define what `code::proof` is required.

# Code with intent

When we think of an algorithm, it's easy to feel overwhelmed by the complexity. And, analogous to the pitfalls of the piano student, a developer may fall prey to the inevitable rabbit holes and distractions that arise. It's tempting to consider  these distractions benign, but, in fact, these actions are frequently the result of techinical anxiety about the task, and are anathema to mastering the technique in question.

## Practice with intent

In learning Bach's Contrapunctus I, the student has various conventional, albeit questionable, methods of approach. 

The student might begin from the start each time. But, as the piece is not yet mastered, the student will inevitably encounter something they cannot play with ease. This is often encountered in the first bar of a Bach piece. If the student plays up to the mistake, and begins again, then they risk becoming adept the start, and becoming increasingly anxious about the end of the piece.  
In an advanced piece such as Contrapunctus I, there are many challenging technical aspects. In particular, the work is polyphonic; Contrapunctus I is a central focus of the string quartet in Vikram Seth's *An Equal Music* [todo citation].  There is a melody; formally referred to as a *subject*.

> todo: figure of subject

This figure reoccurs in bars.... 







> Piano teachers know that students
must practice musically in order to acquire
technique. Both musicality and technique
require accuracy and control. Practically
any technical flaw can be detected in the
music. Nonetheless, many students tend to
practice neglecting the music, preferring to
"work" when no one is around to listen.
Their reasoning is, "I'll practice non-
musically (which is easier because you can
shut off the brain) until I can play it well,
then I'll add the music." This never works
because learning piano is all about training
the brain, not finger calisthenics [@chang_fundamentalspianopractice_2009a]. 

## Mindful coding




In the `varameta::` package, we wish to compare various estimators. There are inputs, an estimator, which is to say a computational process and a similar output, however the devil is in the details, as 

Here are the relationships between inputs, and just some of the estimators, and outputs in `varameta`, presented as a randomised *graph*, a visual representation of a set of nodes, $V$, and the edges, $V \times V$, between them. We understand each function's inputs and outputs, but understanding the way they relate to each other, is often muddled without planning. Our `code::brain`, the way we conceptulaise the code, is disorganised.

```{r codebrain,echo=FALSE,message=FALSE,fig.height=5,fig.cap="`code::brain` before `c::r`."}
library(igraph)

confusing_relationships <- make_graph(~ 
a-+hozo, b-+hozo, m-+hozo, n-+hozo, hozo-+s, hozo-+mean,
a-+wan1, b-+wan1, m-+wan1, n-+wan1, wan1-+s, wan1-+mean,
a-+pren, b-+pren, m-+pren, n-+pren, pren-+s,
q1-+wan3, q3-+wan3, m-+wan3, n-+wan3, wan3-+s, wan3-+s,
q1-+pren, q3-+pren, m-+pren, n-+pren, pren-+s,
a-+wan2, b-+wan2, q1-+wan2, q3-+wan2, m-+wan2, n-+wan2, wan2-+s, wan2-+mean,
a-+bland, b-+bland, q1-+bland, q3-+bland, m-+bland, n-+bland, bland-+s, bland-+mean
           ) 

confusing_relationships %>% plot(
             vertex.color = "grey",
             vertex.frame.color = "darkgrey",
             alpha=0.3,
             vertex.size = 10,
             vertex.label.cex = 0.8,
             edge.arrow.size = 0.4 
             ) +         
  theme(panel.background = element_rect(colour = "#ffffe0"),
        plot.background = element_rect(colour = "#ffffe0")) 


```


The Center for Open Science recommend *preregistering* an experiment, stating what the hypothesis is and how the analyst intends to assess the hypothesis as one safeguard against inadvertent questionable research practices. Analogously, we might think of preregistering our code as providing some `code::proof` of the implementation of our algorithm. In this manuscript, we will think of *c::r*, stating what the intention of an algorithm is, as - todo: complete

To code::register an algorithm:

# `code::registration (c::r)`

In an issue on GitHub:

1. Describe the algorithm's intended purpose.
2. Describe the input parameters and how they will be tested.
3. Describe the output parameters and how they will be tested.

Update the c::r as needed. 

## For complex projects

### On a piece of paper.
  
1. Draw a diagram of the inputs and outputs of the algorithm.
2. Draw where this algorithm fits in the pipeline of the package, if appropriate.

### Write issue

1. Describe the algorithm's intended purpose.
2. Describe the input parameters and how they will be tested.
3. Describe the output parameters and how they will be tested.

### Sketch-diagram 

An image or diagram is very help, if not burdonsomely time consuming (but it often is). This section is only useful for those who enjoy graph theory and visualisation, and is not essential. 

For those comfortable with *graphs* understood as mathematical objects as a set of vertices from $V$, and a set of edges $V \times V$, there are visualisation options where the vertices can be tagged with attributes. The code for constructing the following `nodes` and `edges` dataframes has been omitted for brevity, but all code can be found in this manuscript's associated repository [`neet`](https://github.com/softloud/neet) on GitHub.     

```{r load graphing pkgs, message=FALSE}
library(tidyverse)
library(ggraph)
library(tidygraph)

```

By tagging the nodes in this graph with the attribute `state` in the algorithm: starting with `input` for the estimators, sample quartiles (which we denote $a, q1, m, q3, b$, in order from smallest to largest); `estimator`, a collection of functions that provide statistical methods for preparing summary medians for meta-analysis; and the `output` of the estimator.

```{r nodes,echo=FALSE}

  nodes <- tribble(
  ~node, ~state,
  "a", "input",
  "b", "input",
  "q1", "input",
  "q3", "input",
    "m", "input",
  "n","input",
  "hozo", "estimator",
  "pren_c3", "estimator",
  "pren_c1", "estimator",
  "bland", "estimator",
  "wan_c1", "estimator",
  "wan_c2", "estimator",
  "wan_c3", "estimator",
  "se_median", "output",
  "mean", "output",
  "sd", "output",
  "g_cauchy_c1", "estimator",
  "g_cauchy_c3", "estimator",
  "g_exp_c1", "estimator",
  "g_exp_c3", "estimator",
  "g_norm_c1", "estimator",
  "g_norm_c3", "estimator",
  "g_lnorm_c1", "estimator",
  "g_lnorm_c3", "estimator"
) %>% 
  mutate(state = fct_relevel(state, "input"))


```

```{r}
nodes
```


Edges are specified by a two-column dataframe, `edges`, where each row contains a `to` and `from` vertex identifier, the row number of the `nodes` dataframe.

```{r edges, echo=FALSE}


  edges <- tribble(
  ~from, ~to,
  # hozo estimator
  5,7, 
  6,7,
  1,7,
  2,7,
  7,15,
  7,16,

  # wan1 estimator
  5,11, 
  6,11,
  1,11,
  2,11,
  11,15,
  11,16,
  
  # pren1 estimator
  5,9, 
  6,9,
  1,9,
  2,9,
  9,14,
  
  # bland estimator
  5,10,
  6,10,
  1,10,
  2,10,
  3,10,
  4,10,
  10,15,
  10,16,
  
  # wan2 estimator
  5,12,
  6,12,
  1,12,
  2,12,
  3,12,
  4,12,
  12,15,
  12,16,

  # wan3 estimator
  5,13, 
  6,13,
  3,13,
  4,13,
  13,15,
  13,16,

  # pren3
  5,8, 
  6,8,
  3,8,
  4,8,
  8,14,

  # g_cauchy_c1
  5,17, 
  6,17,
  1,17,
  2,17,
  17,14,
  
  # g_cauchy_c3
  5,18, 
  6,18,
  3,18,
  4,18,
  18,14,

  # g_exp_c1
  5,19, 
  6,19,
  1,19,
  2,19,
  19,14,
  
  # g_exp_c3
  5,20, 
  6,20,
  3,20,
  4,20,
  20,14,

  # g_norm_c1
  5,21, 
  6,21,
  1,21,
  2,21,
  21,14,
  
  # g_norm_c3
  5,22, 
  6,22,
  3,22,
  4,22,
  22,14,
  
  # g_lnorm_c1
  5,23, 
  6,23,
  1,23,
  2,23,
  23,14,
  
  # g_lnorm_c3
  5,24, 
  6,24,
  3,24,
  4,24,
  24,14
  
)

```
```{r}
edges
```

These two dataframes cone be converted into a graph object that is compatible with the `igraph::` package and `tidyverse::` syntax.

```{r}

graph <- tbl_graph(nodes, edges)

```

These can be 

```{r message=FALSE, fig.width=15, fig.height=5, fig.cap="`code::brain` after `c::r`."}


graph %>% 
  mutate(state = fct_relevel(state, "output")) %>% 
  ggraph() +
  geom_edge_link(arrow = arrow(), colour = "lightgrey") + 
  geom_node_label(aes(label = node, colour = state),  
                  size = 5,
                  fill = "lightgrey",
                  alpha = 0.6) +
  theme_graph() +
  hrbrthemes::scale_color_ipsum() +
  theme(legend.position = "none",
        panel.background = element_rect(colour = "#ffffe0"),
        plot.background = element_rect(colour = "#ffffe0")) + 
  scale_y_reverse() + 
  coord_flip()

```

## Test-driven workflow

In order to... doneness ... propose test driven ... implemented as...

```{r, message=FALSE,echo=FALSE}
library(neet)

workflow(0)
```

The model workflow includes three stages, repeated three times, before returning to the start, the `c::r`.

Each phase consists of:

1. `c::r`
2. tests
3. code

## `code::proof` workflow for coding to doneness

The tests vary each time in complexity, so that the complete model cycle consists of ten phases of work:

1. `c::r`
2. neet tests, one per function
3. code
4. `c::r`
5. neet tests, for all inputs for each function
6. code
7. `c::r`
8. tests, and the rest, i.e., any other cases to test for
9. code
7. `c::r`

We use *model* cycle to denote the workflow may be adapted for different use-cases. Our next section steps through each phase of work.

## Coding to `code::proof` toolchain walkthrough

In this section we step through the Coding to `code::proof` for...

- `varameta::`, an in-development analysis that has seen many iterations.  
- `simeta::`
- `repliCATS`


# First cycle: one `neet` test per function 

For the first `code::proof` cycle, we follow the steps:

1. A rough, first-draft `c::r`.
2. Write one `neet` test per function.
3. Write code to make those `neet` tests pass, for each function.

If functions are written, then we only need _confirm_ the functions pass the `neet` tests. 

## `c::r`

```{r}
neet::workflow(1)

```

We begin with a sketch outline of a `c::r`, which we then populate with description in prose, provide some detail of neet testing, before providing case-study.

As our first testing objective is one neet test per function, we need not necessarily work out the details of all possible things to check for. We simply ask, what will it take to check that each function required for the algorithm produces a non-empty thing of expected type? 

```

1. Objective:

2. Inputs:

- [ ] one neet
- [ ] all neets
- [ ] and the rest

3. Outputs:

- [ ] one neet
- [ ] all neets
- [ ] and the rest

```

Note that `- [ ]` will create an unchecked tickbox and `- [x]` will create a tickbox in both GitHub and RMarkdown.

## At least one `neet` test per function

The `neet::` package is part of this manuscript's research compendia, we think of compendia, a collection of components that form a research project. In the case of this manuscript, there is an extension package of `testthat::`.


```{r eval=FALSE}
# install devtools
install.packages(devtools) 

# use devtools to install package from github
devtools::install_github("softloud/neet")

```

## `simeta:: c::r`

```{r}
workflow(1)
```

**Objective:**

Randomly generate meta-analysis data, calculate a confidence interval based on a user-specified estimator (currently only works on `varameta::`), and evaluates if the true parameter falls within; repeat this |trials| number of times, and return the coverage, the proportion of times the true parameter falls within the confidence interval. 

Summarise and provide a dataframe of coverage for different sample sizes, group effect ratios, variability between studies, and distributions. Also provide summary visualisation tools. 

**Inputs:**

User inputs: default to *just* the estimator.  

- [ ] one neet
- [ ] all neets
- [ ] and the rest

**Outputs**

A dataframe of coverage probabilities. 

- [ ] one neet
- [ ] all neets
- [ ] and the rest

## `varameta:: c::r`

## `aggreCAT:: c::r`

# Second cycle: `neet` tests for all inputs 

```{r}
workflow(5)

```

Once we have established there is one `neet` test per function, we might now perform `neet` tests for each possibility for inputs. 

### `c::r`

Update or rewrite `c::r`, as needed.

### `neet` tests for all inputs 

### Coding to `code::proof`


# Third cycle: Testing for doneness


```{r}

```


### `c::r`

Update or rewrite `c::r`, as needed.


### All tests required for purpose

### Coding to `code::proof`


## `c::r` of doneness

```{r}
workflow(10)
```

Under this workflow, the `c::r` is the project overview any developer can return to. So, this final registration not only summarises what has been tested, but also notes any idiosyncrasies for other developers, _expeically_ the future self of the developer, who recalls so little of the code they may as well be a different developer.

At this point, the developer makes a final assessment of whether the stated objective of the implementation of the algorithm. Assess the `code::proof`, through the development process, new ideas, boundaries to check (recall the `logfn` and negative numbers example in Section 5.4[todo: hyperlink]). We might characterise done as when we feel we have sufficient `code::proof`. 

If the algorithm does not yet have sufficient `code::proof`, the cycle can be repeated, and `c::r`s updates. 

# Sufficient `code::proof` of software for doneness

It does not feel a great leap to assume that we none of us will think of every use case, every aspect, of solving an algorithm computationally. Thus, rather than disputing what is or is not _done_, we simply set out to achieve _what we can think of doing_. A `c::r` provides a summary of what we could think of doing to 'further the process of scientific discovery', as Devezer _et al._ might put it [@devezerScientificDiscoveryModelcentric2019], towards our goal.


The first version of this manuscript, itself, is a preregistration of a computational workflow. This workflow is for `team reprocat::` of The repliCATS Project to develop rerproducible algorithms more happily together. The first version provides a description of the workflow for the team; the second version incorporates the team's feedback. This arguably literalises the scientific practices of publishing manuscripts; no manuscript, no matter how definitive, provides everything required for a resaerch project, but may offer particular tools and evidence to further the process of scientific discovery. 


Another researcher software engineer may be cognisant of a tool you are unfamiliar with. By providing the package, and the `c::r`, we provide a method to facilitate what Stodden describes as the _extensibility_ of the algorithm [@stodden_setting_2013]. 


The workflow presented in this manuscript aims to address: `code::xiety`, the anxiety associating with programming; _good enough_ scientific practices, by incorporating automated testing [@wilson_good_2017]; and collaborative benefits. `c::r` provides a means of providing accessibility and transparency to the next developer. _Especially_ if the next developer is your future self.   

# References
